{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "710f25f6-9b0e-48df-84a3-8041802c1c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Sam/git/ml_projects/.venv/bin/python3: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-aiplatform google-cloud-storage scikit-learn xgboost fastapi uvicorn\n",
    "# gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df580c59-da48-4cf0-adbb-403f778ce292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.gapic.schema import predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e1e96e2-f63b-443b-9348-5067be38b6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 1. Config\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa1148d-20b6-46c8-96d5-cbfc27ab4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"project-04642f0b-576e-45fc-81f\"           \n",
    "LOCATION = \"us-central1\"                 \n",
    "BUCKET_NAME = f\"{PROJECT_ID}-vertex-models\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"fraud-xgb-{TIMESTAMP}\"\n",
    "ENDPOINT_DISPLAY_NAME = f\"fraud-endpoint-{TIMESTAMP}\"\n",
    "\n",
    "# Artifact paths\n",
    "ARTIFACT_URI = f\"gs://{BUCKET_NAME}/{MODEL_DISPLAY_NAME}\"\n",
    "CONTAINER_IMAGE = f\"us-docker.pkg.dev/{PROJECT_ID}/vertex-prediction/custom-fastapi:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40182ae-0f3b-488b-a2f3-b808d4666b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 2. Train and Save a simple Model\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 2. Train and Save a simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e723a4bb-c7ab-4746-a2b1-4e0d271f195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9942\n",
      "Test  accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=50_000, n_features=20, n_informative=15,\n",
    "    random_state=42, class_sep=1.2\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = HistGradientBoostingClassifier(max_iter=200, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, model.predict(X_train)))\n",
    "print(\"Test  accuracy:\", accuracy_score(y_test,  model.predict(X_test)))\n",
    "\n",
    "# Save model + preprocessor info\n",
    "model_dir = Path(\"artifacts/model\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(model_dir / \"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Minimal metadata (Vertex AI likes to see this sometimes)\n",
    "metadata = {\"framework\": \"scikit-learn\", \"version\": \"1.5\", \"trained\": TIMESTAMP}\n",
    "with open(model_dir / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae5cee1-68f4-461b-8b07-93f6030fc47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63657534  1.96651418  0.19538661 ... -0.64602269  0.13313867\n",
      "   3.69264014]\n",
      " [-5.97280996  0.52032228 -0.84339535 ... -3.74705627  0.49234605\n",
      "  -0.33216355]\n",
      " [-4.49936943  0.38124791  0.05331708 ... -1.22109478 -0.6326804\n",
      "   2.90264548]\n",
      " ...\n",
      " [-3.6684972   1.36052215  0.84745957 ...  1.54544045 -1.420468\n",
      "  -0.71083699]\n",
      " [ 6.20095328 -0.05635427  5.13914594 ...  1.19260612  0.68431307\n",
      "  -3.45626169]\n",
      " [-5.51872527  6.44380414 -2.879291   ... -3.86224261 -0.57876872\n",
      "  -0.76907594]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469bd129-41a9-476f-b799-50c270973d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 3. Upload model artifacts to GCS\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 3. Upload model artifacts to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b96cef8d-0223-41b0-86dc-1c0106cab138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=7hUsmpzofvUD068c6ANmhmW9IVcJDu&access_type=offline&code_challenge=dWtj06eUiP3nL6UM9pYu93XLsXLyhjhbAR-JY0wGy4w&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [sam.aiexpert2023@gmail.com].\n",
      "Your current project is [project-04642f0b-576e-45fc-81f].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "\n",
      "\n",
      "Updates are available for some Google Cloud CLI components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d85878-117f-4d54-814f-ba039ce95399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you experience problems with multiprocessing on MacOS, they might be related to https://bugs.python.org/issue33725. You can disable multiprocessing by editing your .boto config or by adding the following flag to your command: `-o \"GSUtil:parallel_process_count=1\"`. Note that multithreading is still available even if you disable multiprocessing.\n",
      "\n",
      "Copying file://artifacts/model/metadata.json [Content-Type=application/json]...\n",
      "Copying file://artifacts/model/model.pkl [Content-Type=application/octet-stream]...\n",
      "- [2/2 files][740.0 KiB/740.0 KiB] 100% Done                                    \n",
      "Operation completed over 2 objects/740.0 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r artifacts/model/* gs://{BUCKET_NAME}/{MODEL_DISPLAY_NAME}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d166504-43ed-4a78-805a-7a0b7d85e3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 4. Create FastAPI serving application (prediction server)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 4. Create FastAPI serving application (prediction server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a4d006a-6bc2-4fb8-ad83-7872b2f202b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_code = \"\"\"\\\n",
    "# app/main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "app = FastAPI(title=\"Vertex AI Custom Prediction\")\n",
    "\n",
    "with open(\"/model/model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "class PredictRequest(BaseModel):\n",
    "    instances: List[List[float]]\n",
    "\n",
    "class PredictResponse(BaseModel):\n",
    "    predictions: List[int]\n",
    "    probabilities: List[List[float]]\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictResponse)\n",
    "async def predict(body: PredictRequest):\n",
    "    try:\n",
    "        X = np.array(body.instances, dtype=np.float32)\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(\"Expected 2D array\")\n",
    "\n",
    "        proba = model.predict_proba(X).tolist()\n",
    "        preds = model.predict(X).tolist()\n",
    "\n",
    "        return {\"predictions\": preds, \"probabilities\": proba}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"app/main.py\", \"w\") as f:\n",
    "    f.write(fastapi_code)\n",
    "\n",
    "# Minimal requirements\n",
    "with open(\"app/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\\\n",
    "            fastapi==0.115.0\n",
    "            uvicorn==0.30.6\n",
    "            numpy==1.26.4\n",
    "            scikit-learn==1.5.2\n",
    "            \"\"\")\n",
    "\n",
    "# Very simple Dockerfile\n",
    "dockerfile = f\"\"\"\\\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY /app/requirements.txt requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY /app/main.py main.py\n",
    "\n",
    "COPY /artifacts/model /model\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
    "\"\"\"\n",
    "\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33763287-7922-4bcd-9fc7-e07b4202561c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 5. Build & push container (run these commands manually or via cloudbuild)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 5. Build & push container (run these commands manually or via cloudbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f638a971-0749-41f6-aab0-7b5c81a8ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable compute.googleapis.com --project {PROJECT_ID} --quiet\n",
    "!gcloud services enable appengine.googleapis.com --project {PROJECT_ID} --quiet   # or whichever API it complained about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "294b2920-c924-409f-9c2f-fcb38b8f0753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=BIzWxxETFsGpThHxyFGIJPuuEDp3rI&access_type=offline&code_challenge=BaeoRAea6YJImLnvInqhkz0MVtUduoxiypHnJlHVLgs&code_challenge_method=S256\n",
      "\n",
      "\n",
      "Credentials saved to file: [/Users/Sam/.config/gcloud/application_default_credentials.json]\n",
      "\n",
      "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
      "\n",
      "Quota project \"project-04642f0b-576e-45fc-81f\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e36deb-750c-434e-923a-f124a3865e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [vertex-prediction]\n",
      "Waiting for operation [projects/project-04642f0b-576e-45fc-81f/locations/us/ope\n",
      "rations/19bb590e-52c1-461d-bee1-46b2d074d744] to complete...done.              \n",
      "Created repository [vertex-prediction].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create vertex-prediction \\\n",
    "  --repository-format=docker \\\n",
    "  --location=us \\\n",
    "  --description=\"Repository for Vertex custom FastAPI images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56506821-1b2d-4ece-9675-b03a9e0e3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 11 file(s) totalling 840.4 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://project-04642f0b-576e-45fc-81f_cloudbuild/source/1770268358.837656-b0455d3227364abc9a9f8a43e4f2c910.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/project-04642f0b-576e-45fc-81f/locations/global/builds/9f78f12f-0427-4054-b484-aea338795364].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/9f78f12f-0427-4054-b484-aea338795364?project=492144913879 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"9f78f12f-0427-4054-b484-aea338795364\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://project-04642f0b-576e-45fc-81f_cloudbuild/source/1770268358.837656-b0455d3227364abc9a9f8a43e4f2c910.tgz#1770268359724222\n",
      "Copying gs://project-04642f0b-576e-45fc-81f_cloudbuild/source/1770268358.837656-b0455d3227364abc9a9f8a43e4f2c910.tgz#1770268359724222...\n",
      "/ [1 files][382.6 KiB/382.6 KiB]                                                \n",
      "Operation completed over 1 objects/382.6 KiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/gcb-internal\n",
      "Sending build context to Docker daemon    873kB\n",
      "Step 1/7 : FROM python:3.11-slim\n",
      "3.11-slim: Pulling from library/python\n",
      "0c8d55a45c0d: Pulling fs layer\n",
      "64faa99400e1: Pulling fs layer\n",
      "8cbc47ff628d: Pulling fs layer\n",
      "d85099f0969e: Pulling fs layer\n",
      "64faa99400e1: Verifying Checksum\n",
      "64faa99400e1: Download complete\n",
      "d85099f0969e: Verifying Checksum\n",
      "d85099f0969e: Download complete\n",
      "0c8d55a45c0d: Verifying Checksum\n",
      "0c8d55a45c0d: Download complete\n",
      "0c8d55a45c0d: Pull complete\n",
      "64faa99400e1: Pull complete\n",
      "8cbc47ff628d: Verifying Checksum\n",
      "8cbc47ff628d: Download complete\n",
      "8cbc47ff628d: Pull complete\n",
      "d85099f0969e: Pull complete\n",
      "Digest: sha256:db27ce7778e5f581d5d97812ee577a01a9fffbfa612c47fc521fa684e3389c9b\n",
      "Status: Downloaded newer image for python:3.11-slim\n",
      " ---> 466c0182639b\n",
      "Step 2/7 : WORKDIR /app\n",
      " ---> Running in e29299037527\n",
      "Removing intermediate container e29299037527\n",
      " ---> 95cfab5d0c56\n",
      "Step 3/7 : COPY /app/requirements.txt requirements.txt\n",
      " ---> 6bbefe1dfe25\n",
      "Step 4/7 : RUN pip install --no-cache-dir -r requirements.txt\n",
      " ---> Running in 9b3b081cad99\n",
      "Collecting fastapi==0.115.0 (from -r requirements.txt (line 1))\n",
      "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn==0.30.6 (from -r requirements.txt (line 2))\n",
      "  Downloading uvicorn-0.30.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting numpy==1.26.4 (from -r requirements.txt (line 3))\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting starlette<0.39.0,>=0.37.2 (from fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90.6/90.6 kB 236.3 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.8.0 (from fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting click>=7.0 (from uvicorn==0.30.6->-r requirements.txt (line 2))\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting h11>=0.8 (from uvicorn==0.30.6->-r requirements.txt (line 2))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.5.2->-r requirements.txt (line 4))\n",
      "  Downloading scipy-1.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 187.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.5.2->-r requirements.txt (line 4))\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.2->-r requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.4.0 (from starlette<0.39.0,>=0.37.2->fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.4.0->starlette<0.39.0,>=0.37.2->fastapi==0.115.0->-r requirements.txt (line 1))\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 246.9 MB/s eta 0:00:00\n",
      "Downloading uvicorn-0.30.6-py3-none-any.whl (62 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 214.0 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 154.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 92.5 MB/s eta 0:00:00\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.3/108.3 kB 206.0 MB/s eta 0:00:00\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.1/309.1 kB 245.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 463.6/463.6 kB 301.8 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 256.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.17.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.1/35.1 MB 232.4 MB/s eta 0:00:00\n",
      "Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 kB 244.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 193.4 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.6/113.6 kB 250.3 MB/s eta 0:00:00\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.0/71.0 kB 240.9 MB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, threadpoolctl, numpy, joblib, idna, h11, click, annotated-types, uvicorn, typing-inspection, scipy, pydantic-core, anyio, starlette, scikit-learn, pydantic, fastapi\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.1 click-8.3.1 fastapi-0.115.0 h11-0.16.0 idna-3.11 joblib-1.5.3 numpy-1.26.4 pydantic-2.12.5 pydantic-core-2.41.5 scikit-learn-1.5.2 scipy-1.17.0 starlette-0.38.6 threadpoolctl-3.6.0 typing-extensions-4.15.0 typing-inspection-0.4.2 uvicorn-0.30.6\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 24.0 -> 26.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 9b3b081cad99\n",
      " ---> 1f1c446dd0db\n",
      "Step 5/7 : COPY /app/main.py main.py\n",
      " ---> d764ab206fa8\n",
      "Step 6/7 : COPY /artifacts/model /model\n",
      " ---> d062fd3b440c\n",
      "Step 7/7 : CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
      " ---> Running in f1040c527a9b\n",
      "Removing intermediate container f1040c527a9b\n",
      " ---> 9a3f1410a578\n",
      "Successfully built 9a3f1410a578\n",
      "Successfully tagged us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "PUSH\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 1 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 2 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 3 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 4 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 5 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "a8ff6f8cbdfd: Preparing\n",
      "ERROR: push attempt 6 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 7 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 8 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 9 detected failure, retrying: step exited with non-zero status: 1\n",
      "Pushing us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\n",
      "The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "ERROR: push attempt 10 detected failure, retrying: step exited with non-zero status: 1\n",
      "ERROR: failed to push because we ran out of retries.\n",
      "ERROR\n",
      "ERROR: error pushing image \"us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest\": retry budget exhausted (10 attempts): step exited with non-zero status: 1\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "INFO: The service account running this build projects/project-04642f0b-576e-45fc-81f/serviceAccounts/492144913879-compute@developer.gserviceaccount.com does not have permission to write logs to Cloud Logging. To fix this, grant the Logs Writer (roles/logging.logWriter) role to the service account.\n",
      "\n",
      "1 message(s) issued.\n",
      "\n",
      "BUILD FAILURE: Docker image push failure (\"The push refers to repository [us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi]\n",
      "f5b5f4ba7f38: Preparing\n",
      "c8faca7aab7b: Preparing\n",
      "4381b67f7483: Preparing\n",
      "066bb8de7a59: Preparing\n",
      "56b733462c55: Preparing\n",
      "b69aea4cac7d: Preparing\n",
      "40b88e8d19a2: Preparing\n",
      "dfd9efb4ec4c: Preparing\n",
      "a8ff6f8cbdfd: Preparing\n",
      "name unknown: Repository \"vertex-prediction\" not found\n",
      "\"): Verify that the repositor(y|ies) us-docker.pkg.dev/project-04642f0b-576e-45fc-81f/vertex-prediction/custom-fastapi:latest exists.\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) build 9f78f12f-0427-4054-b484-aea338795364 completed with status \"FAILURE\"\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {CONTAINER_IMAGE} \\\n",
    "  --project {PROJECT_ID} \\\n",
    "  --timeout=20m \\\n",
    "  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b10eac2-3018-43f4-9778-d9c94bcd4acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Build & push the container with one of the commands above ↑\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuild & push the container with one of the commands above ↑\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32709899-3bd7-49bd-ae95-8d0c68ec6115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 6. Upload model to Vertex AI Model Registry\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 6. Upload model to Vertex AI Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b314f2e-aba7-4dd4-b45d-dbc3d89cabfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/492144913879/locations/us-central1/models/8520800599380328448/operations/9111021722525499392\n",
      "Model created. Resource name: projects/492144913879/locations/us-central1/models/8520800599380328448@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/492144913879/locations/us-central1/models/8520800599380328448@1')\n",
      "Model uploaded: projects/492144913879/locations/us-central1/models/8520800599380328448\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=ARTIFACT_URI)\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=ARTIFACT_URI,\n",
    "    serving_container_image_uri=CONTAINER_IMAGE,\n",
    "    serving_container_environment_variables={\n",
    "        \"GUNICORN_CMD_ARGS\": \"--timeout 120\"\n",
    "    },\n",
    "    serving_container_ports=[8080],\n",
    "    description=\"Fraud detection model - HistGradientBoosting\"\n",
    ")\n",
    "\n",
    "print(f\"Model uploaded: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "760a7eb9-d8e9-4864-ba46-c753d9bc01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded: projects/492144913879/locations/us-central1/models/8520800599380328448\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model uploaded: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3296ee3d-7a35-447b-a66b-2c9ee45fb202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 7. Deploy to an endpoint\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "## 7. Deploy to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0b463-bcc2-41a5-bd44-61d6ff39f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/492144913879/locations/us-central1/endpoints/6321750955549261824/operations/8392134632006483968\n",
      "Endpoint created. Resource name: projects/492144913879/locations/us-central1/endpoints/6321750955549261824\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/492144913879/locations/us-central1/endpoints/6321750955549261824')\n",
      "Deploying model to Endpoint : projects/492144913879/locations/us-central1/endpoints/6321750955549261824\n",
      "Deploy Endpoint model backing LRO: projects/492144913879/locations/us-central1/endpoints/6321750955549261824/operations/8226627345700618240\n"
     ]
    }
   ],
   "source": [
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=f\"deployed-{TIMESTAMP}\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=2,\n",
    "    traffic_percentage=100,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint created: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720401a2-ac43-4575-8275-4197b1e3a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## 8. Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d196921-f305-4ea0-a65e-06b566cff062",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    X_test[0].tolist(),\n",
    "    X_test[1].tolist(),\n",
    "    X_test[42].tolist(),\n",
    "]\n",
    "\n",
    "prediction = endpoint.predict(instances=test_instances)\n",
    "\n",
    "print(\"\\nPrediction result:\")\n",
    "print(json.dumps(prediction.predictions, indent=2))\n",
    "\n",
    "# Or lower-level way (good for debugging)\n",
    "instances_proto = [\n",
    "    {\"content\": json.dumps(inst)} for inst in test_instances\n",
    "]\n",
    "\n",
    "response = endpoint.raw_predict(\n",
    "    http_body=predict.HttpBody(\n",
    "        data=json.dumps({\"instances\": test_instances}).encode(\"utf-8\"),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nRaw response:\")\n",
    "print(response.data.decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
